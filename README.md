# 爬蟲與文字探勘實戰工作坊

大數據與人工智慧技術的不斷演化已讓我們無可迴避地需要面對大量且複雜的資料收集、處理、判讀及加值。我們的分析技術能力是否足以應付自如?過去我們在處理資料的眾多環節都是透過人工方式，而且分析的資料也僅是格式化的資料。但現今的統計分析技術已經可以幫助我們透過好的分析工具輔助我們執行自動收集、整理大量資料，如 這次工作坊要教授給各位學員們的python / R 撰寫網路爬蟲，並且透過機器學習/深度學習等方法進一步將分析的資料類型拓展至文字、聲音、影像等非結構化/非格式化資料。

過去社會學相較於其他學門具有「調查數據」、「分析數據」與「解釋數據」等三大優勢，而如今在大數據與人工智能的浪潮下，我們在調查與分析數據的優勢正在逐漸消失。但這並不意味著社會學的專業知識變得越來越不重要。相反的，我們能夠藉由統計分析技術的提升讓已有的社會學訓練變得如虎添翼，也讓自己能夠挖掘到更深且更廣的資料價值。因此，已有社會學基礎或正在接受社會學訓練的大家可以讓自己學習具備程式設計、瞭解演算法特性，以及將分析結果視覺化的能力。

在這次的實戰工作坊中，我們將以各大新聞網站實作網路爬蟲，並且在收集大量的新聞資料後以斷詞、 LDA 等技術進行文字探勘及進一步將分析成果視覺化與加值。歡迎有興趣在未來從事資料科學領域工作的你/妳一起來參加! 



## 課程規劃

### 摘要

- 講題：爬蟲與文字探勘實戰工作坊

- 主講人：游騰林(國泰世華銀行數據分析師)

- 時間：11月30日(六)早上9:00~下午17:00

- 地點：人社二館B213電腦教室

- 學生自行負擔午餐費(統一收費)

- 報名截止日期：11/11(一)，限本系生20人 如截止日未達20人，開放外系生報名

- **有需要跨域自主時數的同學於報名時間內上網報名。**

  報名時間：10/28中午12:00~11/11上午12:00止(如截止日未達20人，開放外系生報名)
  [https://sys.ndhu.edu.tw/SA/XSL_ApplyRWD/ActApply.aspx](https://sys.ndhu.edu.tw/SA/XSL_ApplyRWD/ActApply.aspx?fbclid=IwAR17SQHk3vsS3kF3ZWwlQBI7b0xLSDeL8pbWfszbiShIyOZCo8YG1Dv5JU8)

> 我想了很久，還是讓學生產生興趣還是比較重要!



### 課程大綱

- 講師自我介紹(15 Min)
  - 學經歷
  - slido
  - 課程結束後會提供語法
- 文字資料?(20 min)
  - 文字資料與傳統資料的差異
  - 分享一些文字分析的成功案例(監測網路聲量，輿情偵測, 情緒分析，商品分析)

- 文字分析流程 (20 min)
  - 工具介紹
  - 收集：爬蟲(同時 Show 畫面)
  - 分析：文字探勘(同時 Show 畫面)
- 爬蟲 (3hr)
  - 爬蟲介紹
  - HTML架構
  - Chrome的F12
  - request
  - Beautisoup常用函數
  - 正則表達式
  - pandas
  - 爬蟲實戰
- 分析(3hr)
  - 前處理
    - 斷詞
    - 組字
  - 加值
    - 字典情感評分
    - 訓練模型
  - 分析
    - 單純計算詞頻
    - TF-IDF
  - Jieba斷詞
  - 文字雲



## 講師介紹(0915 - 1000)

- 簡單自我介紹
  - 游騰林
- 學歷
  - 國立東華大學社會系畢業(2010 ~ 2014)
  - 國立東華大學社會所畢業(2014 ~ 2015)
- 經歷
  - 台灣之星客服事業部(2016 ~ 2019)
  - 國泰世華銀行數據部(2019.06 ~ )
- 技能
  - R
  - Python
  - SQL
- 興趣
  - Machine Learning(ML)
  - Deep Learning(DL)
  - Natural Language Processing(NLP)
  - Web Crawler
- 聯絡方式
  - TLYu0419@gmail.com 
  - tlyu0419.github.io
- 開一個 slido 讓學生提問
- 課程結束後會提供語法讓大家可以複習



## 在開始之前

希望大家會對爬蟲跟文字探勘有興趣，並且能享受這個學習的過程

![](https://lh3.googleusercontent.com/NDLIurO2bPwl2cPkSYxO0XXbivgMyNzlyUkMZ8mm2oHK_ZMOROVFHd8-ImyrcUeX2JmwxgetIkwd0CcvMV0LP0JnHZJhR6PhKBbPreSPCg9E6n7QhUAAxjm73IfYXNDz6EgowNy4yjo9qvk0FPM8z9lu4UG0y7HDc-lpF7Qb8nVg4H-0GfmzHPiKIuYPWHaVq-9FJBovO5jNqlf5SAFbVZsu6RJXcdmchpho3O1LtoanQiNWnXealcMfZ2BtULiXkLZvXGzpI264HPzwte7Qb49A2nKu0fjhFAS9pf-aDygcasO3q2Au1vGOdNbY78pzELVLWO0eVrKjvMLYTdbGe9_6Pn7utaBAPPp-fEcCXs9O4edSgnig-ZRk_G5iEtDtDy6AWcs-wsk8FxkWnxpwtev9A7fyJyWlQA5YQp7mFxmLLUlj_L0bxrfVkGUQOyxKvvLQvGhaHnMBtP_eRMIpQWGyMJL0LnFYlFtFysvUBMGLZr3DZToNwBS6YSMvWGag_Ed84MyPZ1xdPeNz9SDixikij5CwbO3P9UYYCMsca2menN99CiRuddW8qLrL3eo7nGXWIaQBzRT4thwlm0KDnJoPQvNdx8EcUi-AGFknKtYPcJg8R4mAYUh0AtvjLRxJHrVx8szSyFyQHUkJdttnlzQ4-Gc4AXfhQc-Erm8DL8AtpmqwECqoB7c=w1436-h929-no)

- 社會學是商品化程度很低的知識，目前你們在學校可能還不太有感覺
- 畢業後，特別是開始找工作時就會開始經歷被拒絕，被否定，會沮喪，但不要因此否定自己的學習歷程

## 課程目標

- [ ] 具備網路爬蟲技術，能夠輕易擷取各大新聞網站的資料內容。
  - 新聞
  - Yahoo 電影短評
  - google play APP 評價

- [ ] 具備文字探勘技術，學會處理與分析非結構化的文本資料。
  - 暫定主題為新興的外送產業



## 為什麼網路爬蟲與文字探勘很重要?

### 什麼是文字資料?

- 四種資料的測量尺度
  - 類別
  - 順序
  - 等距
  - 等比

### 新形態的資料形態

- 大數據的3個V

  - 資料量大
    - 100筆、200筆
    - 千萬筆
  - 形態多樣
    - 文字
    - 影像
    - 影片
  - 即時
    - 批次
    - 即時

  > 傳統分析著重在於 結構化資料 分析，而文字探勘的重點在於如何從非結構化的文字中，萃取出有用 的重要資訊或知識 

### 文字分析範例

> 直接Show畫面

- 想了解 APP 的優缺點? 

  > google play

- 想了解市場的流行商品與網友評論

  > 松果購物網站的商品資訊與網友評論
  >
  > 補充這個拿到Offer

- 政治人物想了解自己與競爭對手的選情

  > Facebook

- 想了解社會對於特定的議題觀點

  > 新聞

- 想約女生去看電影，但怕踩雷?

  > 電影評論

- 民意調查
- 找出關聯議題
- 找出文章正負評
- 文章摘要
- 對話機器人
- 文本分類 
- 判斷情緒

爬蟲之後還可以看這些帳號在哪裡出現過，

- 爬遍所有航空公司的網站，讓使用者只要輸入起訖點就可以知道最便宜的票價在哪裡 

- 爬遍 Ptt、新聞評論，建立輿情分析系統，測風向!

  

### 這些用以前的調查方式不行嗎?

只要調整模型參數就可以靈活適配各種選舉場景，周期短、成本低、樣本廣、效率高

引出問題

- 這些分析傳統的分析可以做到嗎?答案是可以的，傳統會怎麼做?首先我們先設計問卷，找3-5位專家評估信效度，發問卷調查，收集問卷，分析
- 這些流程走完就已經一個月過去了，取網路上的文字資料是不是更快呢?



### 網路爬蟲

- 網路上的資料形態通常都是半/非結構化的資料
  - 結構化資料 
    - 每筆資料都有固定的欄位、固定的格式，方便程式進行後續取用與分析
    - 例如： 資料庫表格中所存放的資料 
  - 半結構化資料 
    - 資料介於結構化資料與非結構化資料之間
    - 資料具有欄位，也可以依據欄位來進行查找，使用方便，但每筆資料的欄 位可能不一致
    - 可以彈性的存放各種欄位格式 的資料 
    - json不用宣告欄位的結尾，可以比 XML更快更有效傳輸資料 
    - 例如：XML, JSON 
  - 非結構化資料 
    - 沒有固定的格式，必須整理以後才能存取
    - 沒有格式的文字、網頁數據 
    - 必須透過ETL (Extract, Transformation, Loading) 工具將資料轉 換為結構化資料才能取用 




### ETL

- 資料抽取、轉換、儲存 (Data ETL) 

  - 原始資料 Raw Data ETL
  - 腳本 ETL Script 
  - 結構化資料 Tidy Data 
  - 網路爬蟲
    - 自動收集資料
    - 將非結構化數據轉變為結構化數據 




## Python 簡介與基本操作(1000-1100)

### 簡介

- 工具介紹：python

- 廣泛的應用在機器學習，深度學習



### 安裝

- Anaconda



### 安裝套件

- pipinstall



### 第一個指令

- print

  ```python
  print('Hello python!')
  ```

- 變數

  ```python
  Name = '游騰林'
  print('Hello ' + Name)
  ```



### 資料形態

- int

  ```python
  a = 10
  print(a * 10) 
  print(a+1)
  ```

- str

  ```python
  a = 'Hello'
  print(a * 10)
  ```

  > 同樣是a，後面的a 會把前面的a覆蓋掉

- time

  ```python
  YMD1 = '2019-03-01'
  YMD2 = datetime.strftime('2019-03-01')
  # 後面的可以做時間運算，前面不能
  ```

  - 順便補充時間格式

- list

  ```python
  names = ['呂先生','莊先生', '黎先生','陳小姐', '田小姐', '黃先生']
  name[0]
  ```

  - 

### for 迴圈

```python
names = ['呂先生','莊先生', '黎先生','陳小姐', '田小姐', '黃先生']
for i in names:
    print(i)
```

- 練習在人名後加上"你好"



### 判斷式

- if_else

- 呂先生是VIP客戶， 要有特殊的招呼語

  ```python
  names = ['呂先生','莊先生', '黎先生','陳小姐', '田小姐', '黃先生']
  for i in names:
      print(i)
  ```

- 練習先生+帥氣，女生+美麗



### 定義函數

- lambda

- def

> 輸入人名自動輸出以上句子





### 正則表達式

- re.search
- re.findall
- 找句子中有沒有出現什麼



### while 迴圈

- while
- 拍拍機器人
- 女朋友心情不好要有一個拍拍機器人



### Try函數

- 女朋友懷疑你是用機器人，所以決定不輸入文字，用數字來測試你
- 讓學生思考為什麼會數字會錯?
- 為了避免錯誤加入 try_except
- 網頁抓資料容易抓不到，為了避免錯誤，要用 try 函數



### 小結

- 說明等一下要講爬蟲
- 簡介以上函數會在爬蟲的哪裡使用
- 收集：爬蟲(同時 Show 畫面)
  - 一個新聞頁面轉成一筆資料
  - 收集新聞連結清單(正則表達式)
  - 收集多個分頁(for迴圈)

- 休息10分鐘
- 有問題可以在slide上發問



## 爬蟲(1100-1200； 1400-1500)

### 爬蟲介紹

- 網路爬蟲是一種機器人，可以幫您自動地瀏覽網際網路並擷取目標資訊
- 自動化爬取目標網站，並按照您的需求 蒐集目標資料 



### 爬蟲眼中的世界 

- 展示網頁原始碼與實際的網頁畫面



### 網頁的組成 

- HTML
- CSS
- JAVA



### HTML架構

- Content
- Structual
- HyperText Markup Language (超文件標示語言)



### 由一群元素 (Elements) 所組成的階層式文件

- 一個元素包含開始標籤、結束標籤、屬性以及內容 

- 以樹狀結構，階層式呈現元素之間的關係 

- 標籤通常是成對出現，有一個開始標籤 (start tag) 與 結束標籤 (end tag)，例如此處的 <h1>

- 標籤內可以有屬性 (attributes) 來說明這個標籤的性質 

- 開始標籤與結束標籤所包含的即為內容 (contents)， 通常為顯示在網頁上的文字 

- 常見的標籤 (Tags) 
  - h1-h6:標題
  - p:段落
  - a: 超連接
  - table：表格
  - tr：表格內的row
  - td：表格內的cell
  - br：換行
  



### 標籤內的屬性 (Attributes)  

  -  標籤內可以有屬性 (attributes) 來說明這個標籤的性 質，通常以 name = "value" 的方式呈現 
  -  一個標籤內也可同時存在多個屬性 
  -  超連結通常都是屬性 href 的值  



### 常見的屬性 (Attributes) 

- class 標籤的類別 (可重複)
- id 標籤的 id (不可重複)
- title 標籤的顯示資訊
- style 標籤的樣式
- data-* 自行定義新的屬性 



### 與 HTML 的初次見面 (1/3)  

-  請先用 Chrome 打開 example_page.html 
-  請再用文字編輯器 (sublime text, 記事本…) 打開 example_page.html 
-  用文字編輯器 (sublime text, notepad…) 打開 example_page.html 後 

##### CSS

- Style
- Presentational
-  Cascading Style Sheets (串接樣式表) 
-  一種用來替 HTML 增加 style 的語言，舉凡修改顏 色、字體大小、字體類型等等，皆由 CSS 完成 
-  請用文字編輯器 (sublime text, notepad…) 打開 example_page.html 後  

##### JS

- Behavioral

#### 小結

-  甚麼元素、標籤、屬性都聽不懂，有沒有懶人包?! 



### 在動手寫爬蟲之前...

-  效益評估!
-  前人種樹，後人乘涼 
    -  e.g. PTT Crawler, 漫畫下載器 
-  想爬的網站是否有 API? 
      -  許多公司會透過 API 來提供乾淨、整齊的資料，並訂定 爬行的規則
      - e.g. Facebook API, Google Maps API 
  -  當一隻有禮貌的爬蟲 
      -  過於頻繁、大量的送出 requests 會造成伺服器的負擔 



### 網路爬蟲三步驟 

1. 透過 requests 取得 HTML 
2. 解析這些資料以取得目標資訊
   - 透過開發者工具，觀察目標資訊的位置
   - 透過 BeautifulSoup 解析 HTML

4. Loop!



### 第一支爬蟲程式

-   發生在一周後的故事。。。
    
    - 關心社會的呂老師對於新興的"外送"產業員很有興趣，想請你幫忙收集各大新聞網站上關於外送員的新聞
    - 進一步詢問過老師後，各大新聞網站包含
      - udn聯合新聞
      - 中時電子報
      - 蘋果日報
      - 自由時報
    - 新聞資訊
      - 標題
      - 時間
      - 內容
      - 網址
    - 剛上完 Python 爬蟲實戰的你，開始回想起今天上課的內容...
      - 練習抓一則新聞
      - 收集新聞清單
      - 透過迴圈收集更多新聞清單
      - 執行爬蟲!



### 如何抓一則新聞

#### requests

```python
import request
url = ''
resp = requests.get(url)
resp
```
- https://zh.wikipedia.org/wiki/HTTP%E7%8A%B6%E6%80%81%E7%A0%81

```python
resp.txt
```

- 我示範中時，給學生練習udn

### BeautifulSoup 登場!  

-   強大且簡單易學的 HTML 解析器
-   將 HTML 轉變成 BeautifulSoup 物件，再用 BeautifulSoup 的函數取得想要的標籤資訊 



### BeautifulSoup 登場! 2

-  標題文字在哪個標籤裡? 

    - Chrome F12

- 讓 BeautifulSoup 更好喝的函數 - find() 

    -   find(tag, attribute, recursive, text, keywords) 
    -   當你想要找一個標籤 
    -   當你想要找一個標籤且屬性為特定值 
      
  -   讓 BeautifulSoup 更好喝的函數 - find_all() 
    
    -    一個 tag 不夠，想找多個怎麼辦?
    -    find_all() 可以幫上忙 
    -    用法跟 find() 一樣，但是回傳的是 Python 的 list 

```python
# import 套件
import requests
from bs4 import BeatifulSoup
# 用 requests 抓取網頁並存在 response
response =
requests.get("https://jimmy15923.github.io/
example_page")
# 用 BS4 解析 HTML 並把結果回傳 soup
soup = BeautifulSoup(response.text,
"lxml")
# 印出 h1 標籤
print(soup.find("h1"))
```

-    找到標籤了! 

    -    其實我們不在乎標籤本身，我們要的是那個躲在標 籤裡面的內容或是屬性! 
    -    加上 .text 後，成功拿到這個標籤的內容  

-    範例 01: BeautifulSoup 的常用函數  

    ```python
    # 找出第一個 td 的標籤
    print(soup.find("td"))
    # 找出第一個 td 的標籤並印出其文字內容
    print(soup.find("td").text)
    # 找出所有 td 的標籤
    print(soup.find_all("td")) 
    # 不指定標籤，但找出所有屬性 class = "zzz" 的標籤
    print(soup.find_all("", {"class":"zzz"}))
    # 找出所有 td 標籤的第三個並找出其中的 a 標籤
    print(soup.find_all("td")[2].find("a"))
    # 找出所有內容等於 python_crawler 的文字
    print(soup.find_all(text="python_crawler"))
    # 找出第一個 a 標籤並印出屬性
    print(soup.find("a").attrs)
    print(soup.find("a")["href"])
    ```

- 抓內文的時候我不想要廣告訊息，先用find，再做第二次find_all



### 將資料放進 DataFrame

```python
import pandas as pd
pd.DataFrame(...)
```



# 休息時間

- 給學生提問



### 收集新聞清單_soup.find

- 用 soup.find 嘗試抓連結

    ```python
    soup.find_all
    ```

    - 同時說明find跟find_all的差別

- 發現會找到許多無關的連結



### 正則表達式_re

-    BeautifulSoup 的調味料: 正則表達式  

-  甚麼是 regular expression? 

    [A-Za-z0-9._]+@[A-Za-z.]+\.(com|edu)+\.tw 
    
    別害怕! 我們一步一步來 



#### 正則表達式的符號

```python
* 前一字元或括號內字元出現0次或多次
a*b* aaaab、aabb、bbb
+ 前一字元或括號內字元出現1次或多次
a+b+ aaabb、abbbb、abbbbb
{m,n} 前一字元或括號內字元出現m次到 n 次 (包含 m, n)
a{1,2}b{3,4} abbb、aabbbb、aabbb
[] 符合括號內的任一字元 
[A-Z]+ APPLE、QWER
\ 跳脫字元 
\.\|\\ .|\
. 符合任何單一字元(符號, 數字, 空格)
a.c auc、abc、a c
```



### 觀察發現怎麼找連結

- 網址後面加上固定位數的數字

- 收集新聞清單_soup.find

    -  regular expression 常用符號 
    
        -  regular expression 使用許多符號來訂定搜尋規則， 要學會使用就必須知道符號的意義 
    
            ```python
            * 前一字元或括號內字元出現0次或多次
            a*b* aaaab、aabb、bbb
            + 前一字元或括號內字元出現1次或多次
            a+b+ aaabb、abbbb、abbbbb
            {m,n} 前一字元或括號內字元出現m次到 n 次 (包含 m, n)
            a{1,2}b{3,4} abbb、aabbbb、aabbb
            [] 符合括號內的任一字元 
            [A-Z]+ APPLE、QWER
            \ 跳脫字元 
            \.\|\\ .|\
            . 符合任何單一字元(符號, 數字, 空格)
            a.c auc、abc、a c
            ```
    
    -  Python 的 re 
    
        -  Python 有內建的 regular expression 函數
    
        - 推薦使用 re.findall() 
    
            ```python
            # 找出所有內容等於 python_crawler 的文字
            pattern =
            "我寫好的 regular expression"
            string =
            "我想要找的字串"
            re.findall(pattern, string)
            ```
    
    - 範例 02-1: *, +, {} 的用法 
    
        -  \* 代表前面的字元可出現零次以上 
        -  \+ 代表前面的字元至少要出現一次以上 
        -  \{m,n} 代表前面的字元可出現 m 次 ~ n 次 (包含) 
    
    
    
    
### 所以要怎麼抓連結?

```python
url = 查詢的分頁
resp = request.get(url)
resp.text
```



```python
# 非常實用的 for loop 寫法，當你使用 find_all 後，
想一口氣把 list 裡面所有 tags 的內容文字取出時，
可以這樣寫
[tag.text for tag in soup.find_all("tag")]

# 用 requests 抓取網頁並存在 response
# 用 BeautifulSoup 解析 HTML 並把結果回傳 soup
response = requests.get("http://yp.518.com.tw/servicelife.html?ctf=10")
soup = BeautifulSoup(response.text,"lxml")
# 抓到所有 li 標籤，且屬性 class=comp_tel，並存在 list 裡
all_phone_text = [tag.text for tag in
soup.find_all("li", {"class":"comp_tel"})]
# 將 list 的所有文字，存為一個 string
all_phone_text = "".join(all_phone_text)
# 用 regular expression 找出所有電話號碼
phone_number = re.findall("0[1-9]+-[0-9]+",
all_phone_text)
```



### 組合以上兩者

- 連結清單
- 新聞內容



# 休息一下

- 給學生提問
- 怎麼設定查詢更多網址



### 抓取更多連結清單

- 展示各個分頁的畫面

- 用 for 迴圈組合

  ```python
  for ...
  concat
  ```

  

### 開多線程加速

```python
多線程
```



### 爬蟲的注意事項

- 網站的流量不是公共資源
- 大量的資源存取有可能會被鎖IP
  - 如果你使用的是學校IP...
- 建議用google colab來做



### 小結 

- requests

  - GET, POST

- BeautifulSoup

  - find_all(tags, attributes)

- regular expression

  - find_all(tags, {attributes: "your_re"}) 

-  爬蟲實戰攻略 - 掌握階層式的架構 

  - 當你使用 find_all() 時，會抓出所有符合條件的標籤 ，但當你先找到一個特定標籤，在使用 find_all () 時，找到的就是在那特定標籤底下所有符合條件的 標籤。 

    ```python
    # 找出所有 tr 標籤
    soup.find_all("tr")
    # 找到 table 標籤底下的所有 tr 標籤
    soup.find("table").find_all("tr")
    # Warning! 不可以這樣寫! Why?
    soup.find_all("table").find("tr")
    ```

-  爬蟲實戰小技巧 

  - 找到目標資訊藏在哪些標籤底下?
    - stripped_strings: 找出 tag 底下所有的文字，且幫你去除 所有空格、換行符號等等，需要用 iterate 的方式取值。  
  -  如何找出沒有任何屬性的標籤? 
    - 指定Class為None就不會抓到同名但有屬性的元素

-  爬蟲實戰練習 
  - 請運用所學過的方法，嘗試爬取以下這兩個網頁的 資訊，並儲存成 CSV
    - 台北票房觀測站-年度排名，請爬取 2016、2017 年度排名 (15 ~ 20 mins)
      - 如果卡住的話，歡迎參考範例 06 的 code
  - yahoo 奇摩電影評論，請選擇在票房排行榜上任何一部您 喜歡的電影，並將其所有的評論文字、評論星等以及該電 影的名稱抓下來 (20 ~ 25 mins)
    - 如果卡住的話，歡迎參考練習 06-2 的 提示 

-  終於把資料都爬下來了  
  -  恭喜你成功養了一隻會實際爬網頁的爬蟲 
  -  隨著不同網頁的變化，你的爬蟲要學會變的更 強大堅韌，才能成功完成任務 
  



# 休息一下

- 給學生提問
- slide



## 文字探勘(1500-1700)

分析：文字探勘(同時 Show 畫面)

### 情境

- 辛苦把資料都抓下來，也把成功資料儲存成結構化的數據，慣老闆突然神來一筆，最近大數據還有甚麼 AI 不是非常紅嗎?為什麼不用這些資料來做一些大數據分析阿! 
  - 你幫我抓下來這麽多資料很好
  - 不同時間有沒有討論不同的事情?
  - 可不可以再多幫忙分析一下，看這些文章都討論了些什麼，以及各自的數量又是多少
- 已經受不了在準備辭呈的你突然想起，Python 爬蟲實戰好像也有玩過一點資料分析耶! 
  - 斷詞
  - 文字雲
  - 分群

### 步驟

- 前處理
  - 斷詞
- 資料量化
  - 詞頻計算
  - 文字矩陣
  - TF-IDF
- 探勘分析
  - 文字雲1
  - 文章分群
  - 文字雲2



### 斷詞(14:00-1500)

####  中文字是需要斷詞的 

-  英文只要用空白就可以斷詞 
-  但中文要如何斷詞? 

- 甚麼是斷詞?
  - 例如：我覺得不行
  - 我 / 覺得 / 不行 ○
  - 我覺 / 得不行 X
- 英文本身就已經用空格做好斷詞這件事，但是中文 真的不行
  - 乖乖的做斷詞吧! 



####  jieba 登場! 

```python
# 載入套件
import jieba
# 把"我覺得可以"作斷詞後
print([x for x in jieba.cut("我覺得不行")])
```

- 把每一條評論斷完詞後，我們就可以得到每條評論 都用了哪些詞 (e.g., 好看、爛片、無聊)
- 再運用統計的方式，計算一下五星評論與一星評論 之間的用詞頻率是否有所不同 



#### 建立專屬詞典

- 預設詞典

- 載入別人建好的詞典

- 手動增添新詞 

- 爬蟲擴增字典 

- 建立斷詞模型

### 文字雲

#### 文字雲1

- 直接繪製文字雲
- 發現都雜在一起，不容易解讀



#### 文字雲2

- 引入tf-idf的文字雲
  - 什麼是tf-idf
- 稍微能看出一些東西了



#### 文字雲3

- 客製化圖形的文字雲



#### 文字雲4

- 按照月份畫文字雲

### 但是實際上事情不會

### 

#### 資料量化

#####  文字雲的應用

- 經過斷詞後，我們得到了詞彙在評論中出現的頻率 ，透過 WordCloud 這個套件可以呈現出這樣的結果 

#####  TF-IDF 

- 直接統計頻率好像太 Low 了，有沒有潮一點的方法? 
- Term Frequency - Inverse Document Frequency
  - TF: 詞頻，該詞在某一文件中出現的次數
  - IDF: 逆向文件頻率，該詞在所有文件中出現的次數 

#####  TF-IDF 的文字雲 

-  透過 TF-IDF 的分析過後，較有意義的詞會更容易 得到更高的權重 



##### Jieba萃取關鍵詞

#####  文字探勘 

-  只要有 Data，不怕沒得玩
  - 換套斷詞方式 (中研院斷詞系統 )
  - word2vec (詞向量) 

#####  票房資料分析 

-  叫好又叫座 
  -  叫好跟叫座有關係嗎?
  - 讓我們用資料找真相 

#####  文字探勘與文字雲 

- 探討評論文字與星等之間的關係
  - 給五顆星的人都寫了甚麼?
  - 給一顆星的人又都寫了甚麼? 
- 文字雲可以美觀地呈現出文字的重要程度 



#####  上午我們學到了…  

- 運用 BeautifulSoup 解析 HTML 網頁
- 運用 requests 發送 GET, POST 請求
- 運用 regular expression 尋找目標資訊
- 運用 pandas 將抓到的資訊儲存為表格
- 運用 sklearn, matplotlib, wordcloud 做簡單的資料分析

#####  偽裝成瀏覽器發送請求  

- 部份網站會判斷你是否為爬蟲程式
- 加上身份識別偽裝成瀏覽器送出請求 

#####  身份識別 User-Agent  

- 敘述瀏覽器使用的系統, 平台, 版本等資訊的字串
- 瀏覽器開發者工具 > Network \> 重新整理網頁  
- 重新整理網頁之後選擇對網頁送出的 request 
-  檢查 Headers 欄位中的 Request Headers 

#####  href 的絕對路徑與相對路徑 

-  絕對路徑 
-  相對路徑  
-  URL 代表檔案在網路上的位置  無法直接對相對路徑送 requests  
  -  請問可以外送十杯咖啡嗎？ 
  -  可以阿，請問要送到哪裡？ 
  -  門口進來左轉的樓梯到三樓右手邊的櫃台 
  -  … 是奧客嗎 

#####  遍歷網站 - 從網頁連結到其他網頁 

-  透過開發者工具查看
- 超連結即是  tag 
-  透過迴圈對所有網址超連結都送出 request 
-  並不是所有網頁的超連結都會出現在首頁
- 只做一次迴圈無法發現其他網頁裡的超連結 
-  看過網站所有超連結 = 看過所有網頁所有超連結
- 紀錄所有需要送 requests 的超連結，直到送過所 有超連結 
-  宣告一個 list 儲存即將要送 request 的網址
- 決定送 request 的中止條件 

#####  遍歷網站 - 更新 wait_list 清單 

-  從 wait_list 中取出網址
- 從 wait_list 中刪除已經取出的網址
- 從 wait_list 中放入新的網址 

#####  解決遍歷網站的迴圈問題 

-  現實中網站的超連結設計 
  -  網頁間可以互相超連結 

- 建立一個已經送過 request 的清單 viewed_list
- wait_list 裏面也不能有重複的 URL  
-  將需要送 request 的超連結存入等待清單
- 紀錄送過的 request 

#####  過濾 href  

```python
import re
＃ 過濾錨點, e.g. #top
check_url_1 = re.match('#.*', url) # True/False
＃ 過濾其他協定, 只接受 http/https
from urllib.parse import urlparse
check_url_2 = urlparse(url).scheme not in
['https', 'http']
＃ 過濾程式碼, e.g. javascript:alert();
check_url_3 = re.match('^javascript.*', url)
```

#####   Google 短網址服務 

- 網站轉址無法透過 URL 判斷是否屬於相同的網域
- 需要送一次 request 再從回傳的 response 中取得 原始 URL 

```python
from tldextract import extract
extract_url = extract(url)
# 判斷是否為 google 短網址
if extract_url.domain == 'goo' or
extract_url.suffix == 'gl':
response = requests.get(extract_url)
print(response.url)
```

#####  爬網站的重點回顧 

- 若是要尋訪所有網頁, 要不斷對找到的網址送 request，並且紀錄尋訪過的網址 
- href 的值要經過過濾，取出符合網址格式
- 分析網址格式與域名確認識自己想要爬的網頁 

#####  甚麼是動態網頁？ 

-  透過程式變動網頁架構
- 需要時間載入資料或是不同使用行為等都會透過 動態網頁的方式呈現 

#####  靜態網頁 vs. 動態網頁 

-  透過 requests.get 拿到的是靜態網頁
- 檢視網頁原始碼看到的是靜態網頁
- 開發者工具 (inspect) 看到的是動態網頁 

##### 網路爬蟲大殺器-Selenium

- Selenium 本來是網頁自動測試工具
- Selenium 經常被拿來處理動態網頁爬蟲
- 因為是模擬操作瀏覽器，速度上會比靜態網頁慢 

- 瀏覽器的大小也會影響網頁結構 
  
-  現代網站根據使用者裝置的大小會有不同的呈現 
  
-   瀏覽器視窗最大化 

   -   一開始打開瀏覽器的時候並非視窗最大化

   -  建議一般使用情況都先將瀏覽器視窗最大化 

      ```python
      from selenium import webdriver
      # 透過指定的瀏覽器 driver 打開 Chrome
      driver =
      webdriver.Chrome('../webdriver/chromedriver')
      # 將瀏覽器視窗最大化
      driver.maximize_window()
      ```

   

#####  了解 tag 之間的關係 

```html
<bookstore>
   <book>
     <title>Harry Potter</title>
     <author>K. Rowling</author>
   </book>
</bookstore>
```

- book 是 title 與 author 的 parent
- title 與 author 都是 book 的 child 

#####  Selenium 定位 tag - XPath 

-  Selenium 還可透過類似路徑寫法的 XPath 定位 tag 
  -  / 從 root 開始選擇 
  - // 從任何地方開始選擇 
  - . 選擇當下這個 node 
  - .. 選擇當下這個 node 的 parent node 
  - @ 選擇 attribute 
  - \* 選擇任何 node 
  - \| OR 

#####  XPath 範例 

-  html > body > 第三個 div > 第一個 div 
-  Beautifulsoup 寫法 
  -  soup.find_all('div')[2].find_all('div')[0] 

#####  Summary 

- 現代網站爬蟲衍生的問題
  - 過濾非必要的 href
  - 解析網域判斷是否要存取該網頁

- 動態網頁
  - 透過 Selenium 取得程式改變後的網頁結構
  - 透過 XPath 定位 tag 

#####  爬蟲不一定要爬網頁 

-  有 API (Application Programming Interface) 的話就 透過 API 做爬蟲 (e.g. Facebook Graph API) 

- 

- 人怎麼收集資料，copy paste

- 但是資料量大的時候怎麼處理

- 網站架構，xml，html，css(美妝師)，

- Chrome的檢查功能

- Network是一個監聽器，可以觀測整個流程

- requests

  - 第一個爬蟲
  - 三行代碼完成
    - import requests
    - res = requests.get('www.yahoo.com.tw')
    - res
    - 呼應到網頁原始碼
    - 複習
  - soup
  - 實作
  - datetime 格式
  - 同樣名稱

  蘋果日報為什麼鎖?想收錢，但不想因此降低流量，所以還是會把內文放進去，目的是希望讓google的爬蟲可以被搜尋到，再用java把內容藏起來

http狀態碼

- 拆分成小步驟，讓後續的維運跟容易些

- - 



# 課後測驗(1600-1630)

1. 範例的正則表達式可以抓到什麼文字
2. request
3. soup
4. 文字雲套件
5. 哪個不是文字探勘的流程
   1. 爬蟲
   2. 

# 解題與提問(1630-1700)





## 

- 小心

- 文字探勘 (1330 ~ 1530)
  - 指定主題數的 LDA 分析
  - 透過模型找出最適分組數
  - 學生練習/提問
- 視覺化(1530 ~ 1730)
  - LDAvis
  - 文字雲
  - 時間序列圖
  - 學生練習/提問

個人

- 下載漫畫

- 學習
  - 蘋果日報
  - 深度學習
    - https://cvdl.cupoy.com/
    - https://cvdl-fileentity.cupoy.com/1st/dailytask/D3/__PDF__?t=1574045799346 
- 對社會有幫助
  - 像老師這樣，經過專業的知識與建議，給社會/政府建議



提問：

1. 如果斷詞斷句是希望保留能理解的最小單位，那為什麼需要n-gram?

> - 保留詞的最小單位，不會再另外對詞做n-gram，另外bert使用的是字的向量
>
> - 如，中華民國如果斷成中華跟民國，各自都可以找到中華民國，但如果斷成中華民國，輸入中華就找不到
>
> - 切成能理解的最小單位(約2-6個字)

2. 半監督式學習，先用非監督式學習找答案，接著再用監督式學習做預測

out of vocabulary

3. kmeans很爛?

   > - Kmeans一定要給K,但每天的主題數量不一定
   > - 社群偵測法(NetworkX 套件)，python- commun
   > -  https://github.com/taynaud/python-louvain 

QA：

1. 如果有跨時間點的資料要怎麼運用?

   > 老師建議：
   >
   > 先用社群偵測法找出多少個群體，接著再建立監督式模型來預測新資料
   >
   > 如果新資料在每個類別的機率都很低，就可以再利用社群算法去做心得分群??但workable需再確認

2. ~~如果把kmeans分成一樣的組數，哪種的效果比較好?~~

3. 用社群算法是希望取代kmeans給指定k的值，但社群演算法卻又產生新的問題，要怎麼給閾值?能不能讓他的input直接是我們算出來的相似度?

1. 老師是先對文章計算cosine_distance，計算出每篇文章彼此間的相似度後，再進行kmeans

2. 計算某一天的新聞，發現3件大事

   



社群偵測法流程：

> 由下而上的分群方式

1. 文章斷詞

2. 計算文章的相似度

3. 設定閾值轉換成是否大於0.3(經驗法則)

4. 將轉換後相似度作d為Input資料給社群演算法

5. 就可以得到各篇文章的主題，數量與熱度

   > 缺點：
   >
   > 1. 閾值不容易設定，是主觀的想法
   >
   > 2. 丟掉了強度的資訊，只保留強度是否大於多少的資訊



訓練測試資料的說明，

切分的原因是模型直接背到答案?(這個說法不太適切)

鴻海



統計學強調見微知著，去做推論，在正確的假設，正確的抽樣,就可以適度的推論

機器學習沒有基本的假設，



發自好奇，享受過程

# 課程計劃





- 可以講一些簡單的機器學習

- 型一錯誤，型二錯誤，不平衡

  - ACC
  - recall
  - precision

  重要性

- TF-IDF是2016前常用的方法

- 同義詞//別名

- 深度學習

  - 辭庫
    - 難以維運:ㄒ
    - 人工作業成本高
    - 無法反映字詞的細微差異
  - 計數
    - PCA
    - SVD
  - 推論手法
    - 

希望大家可以享受學習的過程

- 

2014-2014-2016

自動寫文章







ref：  https://www.slideshare.net/tw_dsconf/python-78691041 

 https://www.slideshare.net/tw_dsconf/ss-73708487 



挑選一些你有興趣的主題，開始你的分析旅程

訪問調查受訪者的職業，現在資料放在人力銀行

一個人的興趣，FB公開

人跟人之間的連結，FB，IG

公司的風險。新聞，人力銀行的職缺

電商需要知道市場流行什麼?傳統問卷，現在可以爬蟲

公司APP設計的優點/缺點是什麼?問卷，現在爬蟲



設定研究目的，找研究方法，收集研究資料，分析



gpt2自動寫文章

gpt2 chinese

hugging face



transformer seq2seq

bert summarizer

- 自動寫評論

- 自動摘要

- - 



- - 





# 補充資料



#####  練習 05: 將抓下來的資訊儲存成表格 (8 mins) 

- 請觀察 518 黃頁網，並將店名、地址及電話三個欄 位抓下來，並存成表格如下  

  - 觀察店名、地址及電話都藏在哪些標籤底下? 有共通的 屬性嗎?

  - 選擇要用 rows 或 columns 來組成 DataFrame 

    ```python
    # 萬年起手式
    response = requests.get("http://yp.518.com.tw/servicelife.html?ctf=10")
    soup = BeautifulSoup(response.text, "lxml")
    # 店家名稱與電話存在同一標籤，地址則在另一標籤
    name_phone = [tag.text for tag in soup.find_all("li",
    class_="comp_tel")]
    address = [tag.text for tag in soup.find_all("li",
    class_="comp_loca")]
    # 運用 re 找到所有的電話號碼，運用 split ，抓到店家名稱
    name_phone_str =
    "".join(name_phone)
    phone = re.findall("[0-9]{2}-[0-9]+", name_phone_str)
    name = [x.split("/")[0].strip() for x in name_phone]
    # 將剛剛處理完的 list，以 dictionary 放進 pd.DataFrame 中
    df = pd.DataFrame({"店名":name,
    "地址": address,
    "電話": phone},columns = ["店名","地址","電話"])
    df.to_csv("csv_results/practice05.csv", index =False,
    encoding="cp950")
    ```

- 收集新聞資料

- 資料格式整理、排序、濾除無關新聞

- 幫忙分群 並看各群的數量

- 討論量的變化

##### 情境

- 自從慣老闆發現你很好用之後，某天又突然跑過來 ，這次是要請你把美食網站上面，所有店家的地址 都抓下來，而且老闆限定要是新北市的喔! 
- 同樣的，這對剛上完 Python 爬蟲實戰的你， 也是小菜一碟 

#####  BeautifulSoup + regular expression

- BeautifulSoup 可以幫您解析 HTML
- regular expression 可以按照您的規則回傳字串 
- 兩個加在一起，就可以按照想要的規則取出目標標籤 

#####  範例 03: BeautifulSoup + regular expression 

```python
# import 套件
import re
# 用 regular expression 找出所有 td 或 tr 標籤
soup.find_all(re.compile("t(d|r)"))
```

- 運用 BeautifulSoup 加上 regular expression，可以抓 到任何給定條件的資訊 

- 在 BeautifulSoup 裡必須使用 re.compile 來寫 pattern

- re.compile 可以用來尋找 tags, attrs 及 contents 

  ```python
  # 找出所有屬性為 class 且值包含至少一個 z 以上的標籤
  soup.find_all("",{"class":re.compile("z+")})
  # 用 regular expression 找出所有包含 python 的
  contents
  soup.find_all("",text=re.compile("python"))
  ```

#####  練習 03: BeautifulSoup + regular expression (10 mins) 

- 請找出範例網頁中所有標籤，其屬性 href 的值包 含資料科學協會的網址 ("http://foundation.datasci.tw/...")
  - 要怎麼找到屬性是 href 的所有標籤?
  - 要如何把 regular expression 加進 BeautifulSoup ? 

- 請觀察518 黃頁網，並找出所有位在新北市的店家 地址
  - 地址的資訊都藏在哪些 tags 底下?
  - 怎麼把 regular expression 用在 contents 上面? 

#####  練習 03-1: 答案 

```python
# 萬年起手式，先 requests 再用 BeautifulSoup 解析
response = requests.get("
https://jimmy15923.github.io/example_page")
soup = BeautifulSoup(response.text, "lxml")
# 不指定標籤，找到屬性 href，值為特定網址
print(soup.find_all(""
, {"href":re.compile(
"http://foundation.datasci.tw")}))
```

#####  練習 03-2: 答案 

```python
# 萬年起手式，先 requests 再用 BeautifulSoup 解析
response =
requests.get("http://yp.518.com.tw/servicelife.html?ctf=10")
soup = BeautifulSoup(response.text, "lxml")
# 找到標籤 li，且屬性 class=comp_loca，內容包含
"新北"
print(soup.find_all("li",{"class":"comp_loca"},
text=re.compile("新北")))
```

#####  情境 

- 慣老闆又出現啦!這次想要請你把某個時段的高鐵 時刻表全部找出來，方便以後出差的時候參考
- 你找到高鐵時刻表網站，開心的想起萬年起手式， 開始 import requests，request.get，這時… 

#####  不會改變的網址 

- 上網時常發現，許多操作像是搜尋、點選等均都是 在同一個網址底下完成，這種網頁該如何用我們之 前學的 requests 來爬取? 

#####  為何網址不會改變? 

- 兩種常見原因
  - 網頁透過 POST 的方式取得資料，先由瀏覽器在背景送 一些資料給 Server，Server 收到 POST 請求後，回傳相對 應的資料
  - 現代的網頁為了提升使用者體驗，會運用 JavaScript, AJAX 等技術，來動態載入資料而不需重新整理網頁 

#####  GET vs. POST 

- GET: 發送 requests，Server 回傳資料
  - URL 會隨著不同的網頁改變
    - 1. http://www.taipeibo.com/yearly/2017
      2. http://www.taipeibo.com/yearly/2016 
  - POST: 發送 requests 並附帶資料，Server 回傳資料
    - 網址不會改變，但是網頁資料會隨著使用者不同的 requests 改變 

#####  如何知道 request methods? 

-  右鍵→檢查 
-  選擇 Network 並勾選底下的 Preserve log 
-  設定要查詢的資料後送出 
-  上方的標籤選 Headers, 可以看到 Request Method 是 POST! 

#####  Form Data 

-  Form Data 是我們剛剛在做 POST 時，傳送給 Server 的資料  
-  Server 看到你的 POST，且透過你附帶的 Form data ，回傳你想要查詢的 HTML 結果 

#####  範例 04: 如何使用 POST 

```python
# 觀察網頁後，找到 option 的值
form_data = {
"StartStation":"2f940836-cedc-41ef-8e28-c2336ac8fe68",
"EndStation":"e6e26e66-7dc1-458f-b2f3-71ce65fdc95f",
"SearchDate":"2017/08/13",
"SearchTime":"20:30",
"SearchWay":"DepartureInMandarin"}
# requests 改用 POST，並放入 form_data
response_post = requests.post("https://www.thsrc.com.tw/tw/
TimeTable/SearchResult", data = form_data)
soup_post = BeautifulSoup(response_post.text, "lxml")
```

#####  練習 04: 如何使用 POST (8 mins)  

- 請運用 POST 方式，找出 2017 年 8 月 14 日 21:30， 南港站到台南站共有幾個班次?

  - 觀察南港、台南站的 option value 是甚麼?

  - 查看班次的資訊都藏在哪些標籤內? 

    ```python
    # 將要查詢的資料寫成 dictionary
    form_data = {
    "StartStation":"2f940836-cedc-41ef-8e28-c2336ac8fe68",
    "EndStation":"9c5ac6ca-ec89-48f8-aab0-41b738cb1814",
    "SearchDate":"2017/08/14",
    "SearchTime":"21:30",
    "SearchWay":"DepartureInMandarin"}
    # requests 改用 POST，並放入 form_data
    response_post = requests.post("https://www.thsrc.com.tw/tw/
    TimeTable/SearchResult", data = form_data)
    soup_post = BeautifulSoup(response_post.text, "lxml")
    # 找出有幾個 td 標籤，屬性為 class=column1
    print(len(soup_post.find_all("td", class_="column1")))
    ```

#####  情境 

- 辛苦抓完所有東西後，慣老闆卻要求說，要把資料 全部都存成表格，這樣我才能方便用 excel 打開還 有編輯啊! 
- 所幸，Python 爬蟲實戰還是有教過怎麼做 

#####  pandas 登場 

- pandas 是一個基於 numpy 的函式庫，不論是用來 讀取、處理數據都非常的簡單方便 
- 處理結構化的數據非常好用!

#####  pandas 的核心 - DataFrame 

-  DataFrame 就是由 rows 跟 columns 所組成的一個 大表格 

#####  DataFrame 的組成 

-  一個 DataFrame 的組成可以分成兩種情形來看 
-  由 columns 組成 

#####  如何用 pandas 儲存爬下來的資料? 

- 如果爬下來的資料是以 row 為單位 

#####  把爬下來的資料變成表格 

-  如果爬下來的資料是以 column 為單位 

#####  常用的資料格式 - CSV 

- Comma-Separated Values，是一種常見的資料格式 ，本身是一個純文字檔，用來記錄欄位的資料，欄 位之間的資料常用逗號作分隔 

  ```python
  # 將剛剛建立好的 DataFrame 存成 csv
  df.to_csv("filename.csv", index=False,
  encoding="cp950")
  ```

- 不須額外增加 index 這個欄位

  - windows 系統，encoding 使用 cp950 (有中文的話用 excel 打開才不會亂碼)，若為 linux 則用 utf-8 

#####  範例 05: 將高鐵時刻表的結果存成 CSV 

```python
# 將要查詢的資料寫成 dictionary
form_data = {
"StartStation":"2f940836-cedc-41ef-8e28-c2336ac8fe68",
"EndStation":"9c5ac6ca-ec89-48f8-aab0-41b738cb1814",
"SearchDate":"2017/08/14",
"SearchTime":"21:30",
"SearchWay":"DepartureInMandarin"}
# requests 改用 POST，並放入 form_data
response_post = requests.post("https://www.thsrc.com.tw/tw/
TimeTable/SearchResult", data = form_data)
soup_post = BeautifulSoup(response_post.text, "lxml")
```

#####  範例 05: 將高鐵時刻表的結果存成 CSV - by columns  

```python
# 找出所有 td 標籤 屬性 class=column1 的內容，並存成 list
train_number = [tag.text for tag in
soup_post.find_all("td", class_="column1")]
# 找出所有 td 標籤 屬性 class=column3 的內容，並存成 list
departure = [tag.text for tag in
soup_post.find_all("td", class_="column3")]
# 找出所有 td 標籤 屬性 class=column4 的內容，並存成 list
arrival = [tag.text for tag in
soup_post.find_all("td", class_="column4")]
# 找出所有 td 標籤 屬性 class=column2 的內容，並存成 list
travel_time = [tag.text for tag in
soup_post.find_all("td", class_="column2")] 
# 建立 DataFrame，把 dictionary 放入，並指定 columns 順序
highway_df = pd.DataFrame(
{"車次":train_number,
"出發時間":departure,
"抵達時間":arrival,
"行車時間":travel_time},
columns = ["車次", "出發時間", "抵達時間", "行車時間"])
```

#####  範例 05: 將高鐵時刻表的結果存成 CSV - by rows  

```python
# 先建立 DataFrame，確定 columns
highway_df = pd.DataFrame(columns = ["車次","出發時間",
"抵達時間", "行車時間"])
# loop 3 次，每次取出一個 row 的 3 個值並存入 DataFrame
for i in range(3):
row = soup_post.find_all("table", class_=
"touch_table")[i]
row_contents = [tag.text for tag in row.find_all("td",
class_= re.compile("column"))]
highway_df.loc[i] = row_contents
# 把建立好的 DataFrame 存成 CSV
# for windows
highway_df.to_csv("data/demo05_highway_schedule.csv", index
= False, encoding = "cp950")
# for linux
highway_df.to_csv("data/demo05_highway_schedule.csv", index
= False, encoding = "utf-8")
```

#####  

#####  用平均星等來預測票房 

- 將電影所有的評論星等作平均，即可獲得該電影的 平均評價
- 透過 Linear Regression，來探討平均星等與票房之 間的關係 

#####  Linear Regression on Scikit-Learn 

```python
# 切 training data 跟 testing data
X_train,X_test,y_train,y_test = train_test_split(
movie_box["平均星等"], np.log10(movie_box["平均票房"]))
# 建立 Regression 模型
reg = LinearRegression()
# 放入 training data 進行訓練
reg.fit(X_train, y_train)
# 用 testing data 做預測
y_pred = reg.predict(X_test)
# 算出 testing data 與預測結果的相關係數
print(pearsonr(y_pred, y_test))
```

#####  為什麼這麼差? (1/2) 

-  評論星等跟票房沒有很強烈的線性關係 
   - plot出來
-  Features 用的太少
   - 還有很多變數可以解釋票房的變異 

-  回歸太難了
   - 試試用分類吧! 

#####  Decision Tree on Scikit-Learn 

```python
# 切 training data 跟 testing data
X_train,X_test,y_train,y_test = train_test_split(
movie_box[["評論數量", "平均星等", "映期"]], movie_box["平均
票房"])
# 建立 Decision Tree 模型
clf = DecisionTreeClassifier(max_depth=3)
# 放入 training data 進行訓練
clf.fit(X_train, y_train)
# 用 testing data 做預測
y_pred = reg.predict(X_test)
# 算出 testing data 與預測結果的準確率
print(accuracy_score(y_pred, y_test))
```

#####  生成的決策樹 

p.134

#####  小結 

-  透過 Decision Tree 的 features importance，我們 可以發現，平均星等根本就不太重要嘛!  
-  你還能想出其他的 features 嗎? 